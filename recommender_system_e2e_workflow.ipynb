{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Reciprocal Recommender Model Retraining and Inference Workflow\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Create Resources](#Create-Resources)\n",
    "1. [Build the Reciprocal Recommender Workflow](#Build-the-Reciprocal-Recommender-Workflow)\n",
    "1. [Run the Workflow](#Run-the-Workflow)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required modules\n",
    "\n",
    "First, we should install and load all the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "import zipfile\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import s3_input\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "id = uuid.uuid4().hex\n",
    "\n",
    "#Create unique names for the AWS Lambda functions to be created. If you change\n",
    "#the default name, you may need to change the Step Functions execution role.\n",
    "processing_function_name = 'query-processing-status'\n",
    "create_preprocessing_function_name = 'create-preprocessing-job'\n",
    "create_batch_pred_function_name = 'create-batch-pred-job'\n",
    "\n",
    "#Create a unique name for the AWS Glue job to be created. If you change the \n",
    "#default name, you may need to change the Step Functions execution role.\n",
    "glue_job_name = 'glue-batch-load-recs'\n",
    "\n",
    "#Model end point name\n",
    "endpoint_name = 'recommender-endpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and push Docker image to ECR\n",
    "\n",
    "Next, we need to build the Docker images for preprocessing, training and inference and push it to ECR to be ready for use by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "algorithm_name=tap-up-recommender-tf\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x training_code/train\n",
    "chmod +x serving_code/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to ap-northeast-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-ap-northeast-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.gpu .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_container_uri = '987654321.dkr.ecr.ap-northeast-1.amazonaws.com/tap-up-recommender-tf:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the IAM roles\n",
    "\n",
    "Next, we'll create fine-grained IAM roles for the Lambda, Glue, and Step Functions resources that we will code. The IAM roles grant the services permissions within your AWS environment.\n",
    "\n",
    "\n",
    "### Add permissions to your notebook role in IAM\n",
    "\n",
    "The IAM role assumed by your notebook requires permission to create and run workflows in AWS Step Functions. If this notebook is running on a SageMaker notebook instance, do the following to provide IAM permissions to the notebook:\n",
    "\n",
    "1. Open the Amazon [SageMaker console](https://console.aws.amazon.com/sagemaker/). \n",
    "2. Select **Notebook instances** and choose the name of your notebook instance.\n",
    "3. Under **Permissions and encryption** select the role ARN to view the role on the IAM console.\n",
    "4. Copy and save the IAM role ARN for later use. \n",
    "5. Choose **Attach policies** and search for `AWSStepFunctionsFullAccess`.\n",
    "6. Select the check box next to `AWSStepFunctionsFullAccess` and choose **Attach policy**.\n",
    "\n",
    "We also need to provide permissions that allow the notebook instance the ability to create an AWS Lambda function and AWS Glue job. We will edit the managed policy attached to our role directly to encorporate these specific permissions:\n",
    "\n",
    "1. Under **Permisions policies** expand the AmazonSageMaker-ExecutionPolicy-******** policy and choose **Edit policy**.\n",
    "2. Select **Add additional permissions**. Choose **IAM**  for Service and **PassRole** for Actions.\n",
    "3. Under Resources, choose **Specific**. Select **Add ARN** and enter `recommender-lambda-role` for **Role name with path*** and choose **Add**. You will create this role later on in this notebook.\n",
    "4. Select **Add additional permissions** a second time. Choose **Lambda** for Service, **Write** for Access level, and **All resources** for Resources.\n",
    "5. Select **Add additional permissions** a final time. Choose **Glue** for Service, **Write** for Access level, and **All resources** for Resources.\n",
    "6. Choose **Review policy** and then **Save changes**.\n",
    "\n",
    "If you are running this notebook outside of SageMaker, the SDK will use your configured AWS CLI configuration. For more information, see [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an execution role in IAM for Step Functions. \n",
    "\n",
    "### Create an Execution Role for Step Functions\n",
    "\n",
    "Your Step Functions workflow requires an IAM role to interact with other services in your AWS environment. \n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/).\n",
    "2. Select **Roles** and then **Create role**.\n",
    "3. Under **Choose the service that will use this role** select **Step Functions**.\n",
    "4. Choose **Next** until you can enter a **Role name**.\n",
    "5. Enter a name such as `StepFunctionsWorkflowExecutionRole` and then select **Create role**.\n",
    "\n",
    "Next, create and attach a policy to the role you created. As a best practice, the following steps will attach a policy that only provides access to the specific resources and actions needed for this solution.\n",
    "\n",
    "1. Under the **Permissions** tab, click **Attach policies** and then **Create policy**.\n",
    "2. Enter the following in the **JSON** tab:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::987654321:role/service-role/AmazonSageMaker-ExecutionRole-20200220T987654\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:StopProcessingJob\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:StopTransformJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:CreateProcessingJob\",\n",
    "                \"sagemaker:CreateTransformJob\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:sagemaker:*:*:*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:DescribeRule\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:PutTargets\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTransformJobsRule\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:InvokeFunction\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:lambda:*:*:function:query-training-status*\",\n",
    "                \"arn:aws:lambda:*:*:function:query-processing-status*\",\n",
    "                \"arn:aws:lambda:*:*:function:create-preprocessing-job*\",\n",
    "                \"arn:aws:lambda:*:*:function:create-batch-pred-job*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:BatchStopJobRun\",\n",
    "                \"glue:GetJobRuns\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:glue:*:*:job/glue-batch-load-recs*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "3. Replace **NOTEBOOK_ROLE_ARN** with the ARN for your notebook that you created in the previous step.\n",
    "4. Choose **Review policy** and give the policy a name such as `StepFunctionsWorkflowExecutionPolicy`.\n",
    "5. Choose **Create policy**.\n",
    "6. Select **Roles** and search for your `StepFunctionsWorkflowExecutionRole` role.\n",
    "7. Under the **Permissions** tab, click **Attach policies**.\n",
    "8. Search for your newly created `StepFunctionsWorkflowExecutionPolicy` policy and select the check box next to it.\n",
    "9. Choose **Attach policy**. You will then be redirected to the details page for the role.\n",
    "10. Copy the StepFunctionsWorkflowExecutionRole **Role ARN** at the top of the Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Execution Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the StepFunctionsWorkflowExecutionRole ARN from above\n",
    "workflow_execution_role = 'arn:aws:iam::987654321:role/StepFunctionsWorkflowExecutionRole'\n",
    "\n",
    "# SageMaker Execution Role\n",
    "# You can use sagemaker.get_execution_role() if running inside sagemaker's notebook instance\n",
    "sagemaker_execution_role = sagemaker.get_execution_role() #Replace with ARN if not in an AWS SageMaker notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Glue IAM Role\n",
    "You need to create an IAM role so that you can create and execute an AWS Glue Job on your data in Amazon S3.\n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/).\n",
    "2. Select **Roles** and then **Create role**.\n",
    "3. Under **Choose the service that will use this role** select **Glue**.\n",
    "4. Choose **Next** until you can enter a **Role name**.\n",
    "5. Enter a name such as `Glue-S3AthenaDDBPipeline` and then select **Create role**.\n",
    "\n",
    "Next, create and attach a policy to the role you created. The following steps attach a managed policy that provides Glue access to the specific S3 bucket holding your data.\n",
    "\n",
    "1. Under the **Permissions** tab, click **Attach policies** and then **Create policy**.\n",
    "2. Enter the following in the **JSON** tab:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PassRole\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"GlueScriptPermissions\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"athena:BatchGetQueryExecution\",\n",
    "                \"athena:GetQueryExecution\",\n",
    "                \"athena:GetQueryResults\",\n",
    "                \"athena:GetQueryResultsStream\",\n",
    "                \"athena:GetWorkGroup\",\n",
    "                \"dynamodb:BatchWriteItem\",\n",
    "                \"glue:GetTable\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ReplicateObject\",\n",
    "                \"s3:RestoreObject\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:athena:*:*:workgroup/*\",\n",
    "                \"arn:aws:dynamodb:*:*:table/*\",\n",
    "                \"arn:aws:glue:*:*:catalog\",\n",
    "                \"arn:aws:glue:*:*:database/reciprocalrec\",\n",
    "                \"arn:aws:glue:*:*:table/reciprocalrec/*\",\n",
    "                \"arn:aws:s3:::sagemaker-ap-northeast-1-987654321\",\n",
    "                \"arn:aws:s3:::sagemaker-ap-northeast-1-987654321/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"Logs\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"athena:GetCatalogs\",\n",
    "                \"logs:Create*\",\n",
    "                \"logs:Put*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "3. Run the next cell (below) to retrieve the specific **S3 bucket name** that we will grant permissions to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Copy the output of the above cell and replace the **occurance** of **BUCKET-NAME** in the JSON text that you entered and similarly copy the database name from Athena and replace the **occurances** of **DATABASE-NAME**.\n",
    "5. Choose **Review policy** and give the policy a name such as `S3AthenaGlueDDBPipelinePolicy`.\n",
    "6. Choose **Create policy**.\n",
    "7. Select **Roles**, then search for and select your `Glue-S3AthenaDDBPipeline` role.\n",
    "8. Under the **Permissions** tab, click **Attach policies**.\n",
    "9. Search for your newly created `S3AthenaGlueDDBPipelinePolicy` policy and select the check box next to it.\n",
    "10. Choose **Attach policy**. You will then be redirected to the details page for the role.\n",
    "11. Copy the **Role ARN** at the top of the Summary tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the AWS-Glue-S3-Bucket-Access role ARN from above\n",
    "glue_role = 'arn:aws:iam::987654321:role/Glue-S3AthenaDDBPipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Lambda IAM Role\n",
    "You also need to create an IAM role so that you can create and execute an AWS Lambda function stored in Amazon S3.\n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/).\n",
    "2. Select **Roles** and then **Create role**.\n",
    "3. Under **Choose the service that will use this role** select **Lambda**.\n",
    "4. Choose **Next** until you can enter a **Role name**.\n",
    "5. Enter a name such as `recommender-lambda-role` and then select **Create role**.\n",
    "\n",
    "Next, attach policies to the role you created. The following steps attach policies that provides Lambda access to DynamoDB, Step Functions, S3 and read-only access to SageMaker.\n",
    "\n",
    "1. Under the **Permissions** tab, click **Attach Policies**.\n",
    "2. In the search box, type **SageMaker** and select **AmazonSageMakerFullAccess** from the populated list.\n",
    "3. In the search box type **AWSLambda** and select **AWSLambdaBasicExecutionRole** from the populated list.\n",
    "4. Create custom policies to access **DynamoDB**, **Systems Manager** and **StepFunctions** and select them.\n",
    "5. Choose **Attach policy**. You will then be redirected to the details page for the role.\n",
    "6. Copy the **Role ARN** at the top of the **Summary**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the recommender-lambda-role role ARN from above\n",
    "lambda_role = 'arn:aws:iam::987654321:role/recommender-lambda-role'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'reciprocal_rec_system'\n",
    "input_key = 'data/input/\n",
    "input_data_source = 's3://{}/{}/{}'.format(bucket, project_name, input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Resources\n",
    "In the following steps we'll create all the lambda functions and the Glue job that are called from the Step Functions workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Lambda Function that polls Preprocessing and Batch Inference jobs to check their status periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_processing_zip_name = 'query_processing_status.zip'\n",
    "query_processing_lambda_source_code = './container/training_code/query_processing_status.py'\n",
    "\n",
    "with zipfile.ZipFile(query_processing_zip_name, mode='w') as zf:\n",
    "    zf.write(query_processing_lambda_source_code, arcname=query_processing_lambda_source_code.split('/')[-1])\n",
    "\n",
    "\n",
    "S3Uploader.upload(local_path=query_processing_zip_name, \n",
    "                  desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                  session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.create_function(\n",
    "    FunctionName=processing_function_name,\n",
    "    Runtime='python3.7',\n",
    "    Role=lambda_role,\n",
    "    Handler='query_processing_status.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': '{}/{}'.format(project_name, query_processing_zip_name)\n",
    "    },\n",
    "    Description='Queries a SageMaker processing job and returns the results',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Lambda function that creates and triggers the Preprocessing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_zip_name = 'create_preprocessing_job.zip'\n",
    "etl_source_code = 'container/training_code/create_preprocessing_job.py'\n",
    "\n",
    "with zipfile.ZipFile(etl_zip_name, 'w') as zf:\n",
    "    zf.write(etl_source_code, arcname=etl_source_code.split('/')[-1])\n",
    "\n",
    "S3Uploader.upload(local_path=etl_zip_name, \n",
    "                  desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                  session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.create_function(\n",
    "    FunctionName=create_preprocessing_function_name,\n",
    "    Runtime='python3.7',\n",
    "    Role=lambda_role,\n",
    "    Handler='create_preprocessing_job.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': '{}/{}'.format(project_name, etl_zip_name)\n",
    "    },\n",
    "    Description='Triggers the preprocessing job that makes data to be suitable for training',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Lambda function that creates and triggers the Batch Inference job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred_zip_name = 'create_batch_pred_job.zip'\n",
    "batch_pred_source_code = 'container/training_code/create_batch_pred_job.py'\n",
    "\n",
    "with zipfile.ZipFile(batch_pred_zip_name, 'w') as zf:\n",
    "    zf.write(batch_pred_source_code, arcname=batch_pred_source_code.split('/')[-1])\n",
    "\n",
    "S3Uploader.upload(local_path=batch_pred_zip_name, \n",
    "                  desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                  session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.create_function(\n",
    "    FunctionName=create_batch_pred_function_name,\n",
    "    Runtime='python3.7',\n",
    "    Role=lambda_role,\n",
    "    Handler='create_batch_pred_job.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': '{}/{}'.format(project_name, batch_pred_zip_name)\n",
    "    },\n",
    "    Description='Triggers the inference job that generates the recommendations',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Glue Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_script_location = S3Uploader.upload(local_path='./container/training_code/glue_batch_load_recs.py',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                               session=session)\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_job(\n",
    "    Name=glue_job_name,\n",
    "    Description='PySpark job to extract the parquet data from S3 and load it to DynamoDB',\n",
    "    Role=glue_role,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "#    MaxCapacity=30,\n",
    "    GlueVersion='1.0',\n",
    "    WorkerType='G.1X',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Recommender Estimator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_estimator = sagemaker.estimator.Estimator(image_name=training_container_uri,\n",
    "                                    role=sagemaker_execution_role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.p2.8xlarge',\n",
    "                                    hyperparameters={'vector_size': 50, 'epoch_count': 40, 'batch_value': 32768},\n",
    "                                    output_path='s3://{}/{}/data/model'.format(bucket, project_name),\n",
    "                                    metric_definitions=[{'Name': 'train:loss', 'Regex': '.*loss:\\\\s*(\\\\S+).*'}],\n",
    "                                    enable_sagemaker_metrics=True,\n",
    "                                    input_mode= 'File')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Reciprocal Recommender Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the workflow input schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "    'PreprocessingJobName': str,\n",
    "    'TrainingJobName': str,\n",
    "    'ProcessingLambdaFunctionName': str,\n",
    "    'BatchPredJobName': str,\n",
    "    'CreateBatchPredLambdaFunctionName': str,\n",
    "    'CreatePreprocessingLambdaFunctionName': str,\n",
    "    'GlueBatchJobName': str,\n",
    "    'ModelName': str,\n",
    "    'S3ModelPath': str,\n",
    "    'S3PreprocessedPath': str,\n",
    "    'S3RecommendationsPath': str,\n",
    "    'EndpointName': str,\n",
    "    'DoTraining': bool,\n",
    "    'DoPreprocessing': bool,\n",
    "    'DoBatchRecommend': bool,\n",
    "    'CreateNewEndpoint': bool\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipelines in the workflow in reverse starting from the end (inference, training and preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "### Create a Batch Prediction Step\n",
    "Next, we create a batch prediction step that generates the recommendations for all the users in the dataset and saves the results in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred_configuration = dict(\n",
    "    JobName=execution_input['BatchPredJobName'],\n",
    "    IAMRole=sagemaker_execution_role,\n",
    "    LocalStorageSizeGB=50,\n",
    "    S3InputDataPathModelData = execution_input['S3ModelPath'],\n",
    "    S3OutputDataPath=execution_input['S3RecommendationsPath'],\n",
    "    EcrContainerUri=training_container_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batch_pred_job_step = steps.compute.LambdaStep(\n",
    "    \"Create Batch Inference Job\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['CreateBatchPredLambdaFunctionName'],\n",
    "        \"Payload\": {  \n",
    "           \"Configuration\": batch_pred_configuration\n",
    "        }\n",
    "    },\n",
    "    result_path='$.CreateBatchPredLambdaResult'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lambda step to query BatchPred job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_step_batch_pred = steps.compute.LambdaStep(\n",
    "    'Query Batch Inference Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['ProcessingLambdaFunctionName'],\n",
    "        \"Payload\":{\n",
    "            \"ProcessingJobName.$\": \"$.BatchPredJobName\"\n",
    "        }\n",
    "    },\n",
    "    result_path='$.BatchPredLambdaResult'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a wait state for 60s before querying BatchPred every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_batch_pred_job_wait_state = steps.states.Wait(\n",
    "    \"Wait-2: 60 secs\",\n",
    "    seconds=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a workflow Success Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_step = steps.states.Succeed(\n",
    "    'Recommender Workflow Succeeded',\n",
    "    comment='Final state'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BatchPrediction Failure Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred_fail_step = steps.states.Fail(\n",
    "    \"Batch Inference Failed\",\n",
    "    comment = \"Could not generate recommendations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a batch recommendations generation step with AWS Glue\n",
    "In the following cell, we create a Glue step thats runs an AWS Glue job. The Glue job extracts the recommendations data from the parquet files in S3, processes the data in the required format if required and then saves the data to DynamoDB. Glue is performing this extraction, transformation, and load (ETL) in a serverless fashion, so there are no compute resources to configure and manage. See the [GlueStartJobRunStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.GlueStartJobRunStep) Compute step in the AWS Step Functions Data Science SDK documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_recs_to_ddb_step = steps.GlueStartJobRunStep(\n",
    "    'Load DDB Recommendations',\n",
    "    parameters={\"JobName\": execution_input['GlueBatchJobName'],\n",
    "                \"Arguments\":{\n",
    "                    '--S3_SOURCE': execution_input['S3RecommendationsPath'],\n",
    "                    '--DDB_DEST': 'recommendations'}\n",
    "               }\n",
    ")\n",
    "load_recs_to_ddb_step.next(success_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BatchPred Choice State Step \n",
    "In the following cell, we create a choice step in order to build a dynamic workflow. This choice step branches based off of the results of our Query Batch Predition Results step: did the BatchPred job fail or should the results be loaded into DynamoDB? Otherwise should the workflow wait if BatchPred job is still running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_job_choice_batch_pred = steps.states.Choice(\n",
    "    \"Check Batch Inference Status\"\n",
    ")\n",
    "\n",
    "batch_pred_failed = steps.choice_rule.ChoiceRule.StringEquals(variable=lambda_step_batch_pred.output()['BatchPredLambdaResult']['Payload']['ProcessingJobStatus'], value='Failed')\n",
    "batch_pred_running = steps.choice_rule.ChoiceRule.StringEquals(variable=lambda_step_batch_pred.output()['BatchPredLambdaResult']['Payload']['ProcessingJobStatus'], value='InProgress')\n",
    "batch_pred_finished = steps.choice_rule.ChoiceRule.StringEquals(variable=lambda_step_batch_pred.output()['BatchPredLambdaResult']['Payload']['ProcessingJobStatus'], value='Completed')\n",
    "\n",
    "check_job_choice_batch_pred.add_choice(\n",
    "    rule = batch_pred_running,\n",
    "    next_step=lambda_step_batch_pred\n",
    ")\n",
    "\n",
    "check_job_choice_batch_pred.add_choice(\n",
    "    rule = batch_pred_failed,\n",
    "    next_step = batch_pred_fail_step\n",
    ")\n",
    "\n",
    "check_job_choice_batch_pred.add_choice(\n",
    "    rule = batch_pred_finished,\n",
    "    next_step=load_recs_to_ddb_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link all the Steps Together\n",
    "Finally, create your workflow definition by chaining all of the steps together that we've created. See [Chain](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.states.Chain) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_workflow_definition = steps.Chain([\n",
    "    create_batch_pred_job_step, \n",
    "    lambda_step_batch_pred,\n",
    "    check_batch_pred_job_wait_state,\n",
    "    check_job_choice_batch_pred\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an entrypoint to the inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_entry_checkpoint = steps.states.Choice(\n",
    "    \"Do Inference?\"\n",
    ")\n",
    "\n",
    "skip_batch_pred = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.DoBatchRecommend', value=False)\n",
    "do_batch_pred = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.DoBatchRecommend', value=True)\n",
    "\n",
    "inference_entry_checkpoint.add_choice(\n",
    "    rule = skip_batch_pred,\n",
    "    next_step=success_step\n",
    ")\n",
    "\n",
    "inference_entry_checkpoint.add_choice(\n",
    "    rule = do_batch_pred,\n",
    "    next_step=inference_workflow_definition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "### Create a SageMaker Training Step \n",
    "\n",
    "In the following cell, we create the training step and pass the estimator we defined above. See  [TrainingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "training_step = steps.TrainingStep(\n",
    "    'Train Model',\n",
    "    data={\n",
    "        'training': s3_input(execution_input['S3PreprocessedPath']),\n",
    "    },\n",
    "    estimator=skipgram_estimator,\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True,\n",
    "    result_path='$.TrainingJobResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Model Step \n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    instance_type='ml.m5.12xlarge',\n",
    "    input_path = '$.TrainingJobResults',\n",
    "    result_path='$.ModelStepResults',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint Configuration Step\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.12xlarge',\n",
    "    result_path='$.EndpointConfigResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model Endpoint Step\n",
    "In the following cell, we create the Endpoint step to deploy the new model as a managed API endpoint, creating a new SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_step = steps.EndpointStep(\n",
    "    'Create Model Endpoint',\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    update=False,\n",
    "    result_path='$.EndpointUpdateResults'\n",
    ")\n",
    "create_endpoint_step.next(inference_entry_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Model Endpoint Step\n",
    "In the following cell, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_endpoint_step = steps.EndpointStep(\n",
    "    'Update Model Endpoint',\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    update=True,\n",
    "    result_path='$.EndpointUpdateResults'\n",
    ")\n",
    "update_endpoint_step.next(inference_entry_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint Choice State Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_endpoint_choice = steps.states.Choice(\n",
    "    \"Check Endpoint Status\"\n",
    ")\n",
    "\n",
    "do_endpoint_update = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.CreateNewEndpoint', value=False)\n",
    "do_endpoint_create = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.CreateNewEndpoint', value=True)\n",
    "\n",
    "check_endpoint_choice.add_choice(\n",
    "    rule = do_endpoint_update,\n",
    "    next_step=update_endpoint_step\n",
    ")\n",
    "\n",
    "check_endpoint_choice.add_choice(\n",
    "    rule = do_endpoint_create,\n",
    "    next_step=create_endpoint_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link all the Training steps together\n",
    "Finally, create your workflow definition by chaining all of the training pipeline steps together that we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_workflow_definition = steps.Chain([\n",
    "    training_step,\n",
    "    model_step,\n",
    "    endpoint_config_step,\n",
    "    check_endpoint_choice\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an entrypoint to the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entry_checkpoint = steps.states.Choice(\n",
    "    \"Do Training?\"\n",
    ")\n",
    "\n",
    "skip_training = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.DoTraining', value=False)\n",
    "do_training = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.DoTraining', value=True)\n",
    "\n",
    "train_entry_checkpoint.add_choice(\n",
    "    rule = skip_training,\n",
    "    next_step=inference_entry_checkpoint\n",
    ")\n",
    "\n",
    "train_entry_checkpoint.add_choice(\n",
    "    rule = do_training,\n",
    "    next_step=train_workflow_definition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "### Create a Preprocesing Step\n",
    "\n",
    "In the following cell, we trigger the preprocessing step using an AWS Lambda function and pass the filters for preprocessing. See  [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing_configuration = dict(\n",
    "    JobName=execution_input['PreprocessingJobName'],\n",
    "    IAMRole=sagemaker_execution_role,\n",
    "    LocalStorageSizeGB=50,\n",
    "    S3InputDataPath=input_data_source,\n",
    "    S3OutputDataPath=execution_input['S3PreprocessedPath'],\n",
    "    EcrContainerUri=training_container_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_preprocessing_job_step = steps.compute.LambdaStep(\n",
    "    \"Create Preprocessing Job\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['CreatePreprocessingLambdaFunctionName'],\n",
    "        \"Payload\": {  \n",
    "           \"Configuration\": data_preprocessing_configuration\n",
    "        }\n",
    "    },\n",
    "    result_path='$.CreatePreprocessingLambdaResult'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a lambda step to query Preprocessing status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_step_preprocessing = steps.compute.LambdaStep(\n",
    "    'Query Preprocessing Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['ProcessingLambdaFunctionName'],\n",
    "        \"Payload\":{\n",
    "            \"ProcessingJobName.$\": \"$.PreprocessingJobName\"\n",
    "        }\n",
    "    },\n",
    "    result_path='$.PreprocessingLambdaResult'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a wait state for 60s before querying Preprocessing every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_preprocessing_job_wait_state = steps.states.Wait(\n",
    "    \"Wait-1: 60 secs\",\n",
    "    seconds=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing Failure Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fail_step = steps.states.Fail(\n",
    "    'Preprocessing Failed',\n",
    "    comment='Error while preprocessing data for training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessing Choice State Step \n",
    "In the following cell, we create a choice step in order to build a dynamic workflow. This choice step branches based off of the results of our Query Preprocessing Results step: did the Preprocessing job fail or if successful, should the workflow proceed to Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_job_choice_preprocessing = steps.states.Choice(\n",
    "    \"Check Preprocessing Status\"\n",
    ")\n",
    "\n",
    "preprocessing_failed = steps.choice_rule.ChoiceRule.StringEquals(variable=lambda_step_preprocessing.output()['PreprocessingLambdaResult']['Payload']['ProcessingJobStatus'], value='Failed')\n",
    "preprocessing_running = steps.choice_rule.ChoiceRule.StringEquals(variable=lambda_step_preprocessing.output()['PreprocessingLambdaResult']['Payload']['ProcessingJobStatus'], value='InProgress')\n",
    "preprocessing_finished = steps.choice_rule.ChoiceRule.StringEquals(variable=lambda_step_preprocessing.output()['PreprocessingLambdaResult']['Payload']['ProcessingJobStatus'], value='Completed')\n",
    "\n",
    "check_job_choice_preprocessing.add_choice(\n",
    "    rule = preprocessing_running,\n",
    "    next_step=lambda_step_preprocessing\n",
    ")\n",
    "\n",
    "check_job_choice_preprocessing.add_choice(\n",
    "    rule = preprocessing_failed,\n",
    "    next_step = preprocessing_fail_step\n",
    ")\n",
    "\n",
    "check_job_choice_preprocessing.add_choice(\n",
    "    rule = preprocessing_finished,\n",
    "    next_step=train_entry_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking all steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_workflow_definition = steps.Chain([\n",
    "    create_preprocessing_job_step, \n",
    "    lambda_step_preprocessing,\n",
    "    check_preprocessing_job_wait_state,\n",
    "    check_job_choice_preprocessing,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an entrypoint to the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_entry_checkpoint = steps.states.Choice(\n",
    "    \"Do Preprocessing?\"\n",
    ")\n",
    "\n",
    "skip_preprocessing = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.DoPreprocessing', value=False)\n",
    "do_preprocessing = steps.choice_rule.ChoiceRule.BooleanEquals(variable='$.DoPreprocessing', value=True)\n",
    "\n",
    "preprocessing_entry_checkpoint.add_choice(\n",
    "    rule = skip_preprocessing,\n",
    "    next_step=train_entry_checkpoint\n",
    ")\n",
    "\n",
    "preprocessing_entry_checkpoint.add_choice(\n",
    "    rule = do_preprocessing,\n",
    "    next_step=preprocess_workflow_definition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Workflow\n",
    "Create your workflow using the workflow definition above, and render the graph with [render_graph](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.render_graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_workflow = Workflow(\n",
    "    name='End2End-Routine-{}'.format(id),\n",
    "    definition=steps.Chain([preprocessing_entry_checkpoint]),\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the workflow with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = e2e_workflow.execute(\n",
    "    inputs={\n",
    "    'ProcessingLambdaFunctionName': processing_function_name,\n",
    "    'CreatePreprocessingLambdaFunctionName': create_preprocessing_function_name,\n",
    "    'CreateBatchPredLambdaFunctionName': create_batch_pred_function_name,        \n",
    "    'PreprocessingJobName': 'user-transform-etl-{}'.format(id),\n",
    "    'TrainingJobName': 'train-{}'.format(id),\n",
    "    'ModelName': 'recommender-model-{}'.format(id),\n",
    "    'EndpointName': endpoint_name,\n",
    "    'BatchPredJobName': 'recommender-batch-transform-{}'.format(id),\n",
    "    'GlueBatchJobName': glue_job_name,\n",
    "    'S3ModelPath': 's3://{}/{}/data/model/train-{}/output'.format(bucket, project_name, id),\n",
    "    'S3PreprocessedPath': 's3://{}/{}/data/train/preprocessed-{}'.format(bucket, project_name, id),\n",
    "    'S3RecommendationsPath': 's3://{}/{}/data/output/recommendations-{}'.format(bucket, project_name, id),\n",
    "    'DoPreprocessing':True,\n",
    "    'DoTraining':True,\n",
    "    'DoBatchRecommend':True,\n",
    "    'CreateNewEndpoint': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress). This generates a snapshot of the current state of your workflow as it executes. This is a static image therefore you must run the cell again to check progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "When you are done, make sure to clean up your AWS account by deleting resources you won't be reusing. Uncomment the code below and run the cell to delete the Lambda functions, the Glue Job and the Step Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lambda_client.delete_function(FunctionName=processing_function_name)\n",
    "#lambda_client.delete_function(FunctionName=create_preprocessing_function_name)\n",
    "#lambda_client.delete_function(FunctionName=create_batch_pred_function_name)\n",
    "#glue_client.delete_job(JobName=glue_job_name)\n",
    "#e2e_workflow.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
